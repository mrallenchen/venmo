{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install venmo-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "# ^^ note this was done in terminal and did not work within the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from langdetect import detect\n",
    "import datetime\n",
    "import re\n",
    "import emoji\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pickle.load(open(\"Data/transactions.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = pickle.load(open(\"Data/user_list.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions is a list of transaction objects (venmo API)\\\n",
    "# User_list is a list of user objects (venmo API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286464"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the transactions into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1 - pull out all the relevant attributes from the transaction objects\n",
    "Note that the actor and target attributes themselves are each of objects of the user class. For now, just need to pull the id attribute from there to match up ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_id = [x.id for x in transactions]\n",
    "payment_id = [x.payment_id for x in transactions]\n",
    "date_completed = [x.date_completed for x in transactions]\n",
    "date_created = [x.date_created for x in transactions]\n",
    "date_updated = [x.date_updated for x in transactions]\n",
    "payment_type = [x.payment_type for x in transactions]\n",
    "amount = [x.amount for x in transactions]\n",
    "audience = [x.audience for x in transactions]\n",
    "status = [x.status for x in transactions]\n",
    "note = [x.note for x in transactions]\n",
    "device_used = [x.device_used for x in transactions]\n",
    "actor_id = [x.actor.id for x in transactions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cannot use a list comprehension for target_id because some transactions do not have a target (e.g. cancelation. see transactions 26176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "target_id = []\n",
    "target_issue = []\n",
    "for x in transactions:\n",
    "    try:\n",
    "        target_id.append(x.target.id)\n",
    "    except:\n",
    "        target_issue.append([counter, x.id])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine all series into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [story_id, payment_id, date_completed, date_created, date_updated, payment_type, amount, audience, status, note, device_used, actor_id, target_id]\n",
    "column_names = {0:'story_id', 1:'payment_id', 2:'date_completed', 3:'date_created', 4:'date_updated', 5:'payment_type', 6:'amount', 7:'audience', 8:'status', 9:'note', 10:'device_used', 11:'actor_id', 12:'target_id'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(frames)\n",
    "df = df.T\n",
    "df = df.rename(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(Series):\n",
    "    '''\n",
    "    Need to use a special function because some entries will say 'None', because not all transactions are completed.\n",
    "    So, using normal datetime conversions wont work.\n",
    "    '''\n",
    "    converted = []\n",
    "    counter = 0\n",
    "    for x in Series:\n",
    "        try:\n",
    "            converted.append(datetime.datetime.fromtimestamp(x))\n",
    "        except:\n",
    "            converted.append(None)\n",
    "        counter += 1\n",
    "    return pd.to_datetime(converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date_completed = convert_date(df.date_completed)\n",
    "df.date_created = convert_date(df.date_created)\n",
    "df.date_updated = convert_date(df.date_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/clean_df.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to lookup any users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = dict()\n",
    "for user in user_list:\n",
    "    users[user.id] = user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert everything to lowercase\n",
    "#remove punctuations\n",
    "#correct minor mispelling with multiple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove punctuation, remove words containing numbers, and\n",
    "    for letters that show up 3 or more times consecutively, convert to just one\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[\\.\\,\\!\\?\\:\\;\\-\\=\\#\\$\\&\\(\\)\\+\\/\\/\\@]', '', text) #note that '_' is preserved to be handled later\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r\"([a-z])\\1\\1+\",r\"\\1\\1\",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_r1 = [clean_text_round1(x) for x in df.note]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's talk Emojis\n",
    "\n",
    "Many challenges with emojis. See three sample issues below\n",
    "\n",
    "Problem 1: Emoji characters are typed without spaces:\n",
    "'ugh taxes😬' <- Emoji is tacked onto another word. Need to add a space to separate them \\\n",
    "'🥪🥪🥪' <- Oftentime emojis will be repeated for emphasis. Need to add a space so that it is not treated as one word\\\n",
    "\n",
    "    **Solution**: This can be solved by adding a space prior to every emoji\n",
    "\n",
    "\n",
    "Problem 2: Sometimes emojis are used like letters.\n",
    "'🍞🍗🍞' <-  This example uses emojis to create a chicken sandwich, by using :bread::poultry_leg::bread:\n",
    "\n",
    "    **Solution**: One solution is to leave the emojis as is, and this will then be treated as one word. This would only work if emojis are left in as characters, and not demojized (translated).\n",
    "\n",
    "Problem: \n",
    "'🍿🍿🍿' <- popcorn is meaningful, and i want this entry to be considered the same as 'popcorn'. To make this interpretable, need to get emojis into words\n",
    "\n",
    "    **Solution**: The emoji library can demojize into :popcorn::popcorn::popcorn:\n",
    "\n",
    "\n",
    "**Options**:\n",
    "\n",
    "**A. Spaced_notes:** Keep emojis, with addition of spaces between emojis  \n",
    "**B. Defined:** add the demojis, with spaces between emojis and demojis  \n",
    "**C. Singled_defined:** add demojis, without spaces between emoji and demoji (but between one set of emoji-demoji and the next)  \n",
    "**D. Demoji_notes:** translate the emojis into demojis, removing the emojis. This may be better for traditional natural language processing techniques that are not accustomed to one character words (which an emoji is)  \n",
    "**E. Single_demoji_notes:** same as demoji_notes, but keeping the emoji-demoji together without spaces  \n",
    "**F. Keep emojis, no spaces** <- listing this for completeness, but will not use given that it isn't going to help much. \n",
    "\n",
    "**Champion Model:** After trying a few things out, developed a custom processing technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spaced_Notes\n",
    "- Keep emojis, with addition of spaces between emojis.\n",
    "- This prevents emojis that are listed together from being treated as one \"word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spaced_emoji(text):\n",
    "    emoji_list = emoji.emoji_lis(text)\n",
    "    new_text = str()\n",
    "    if emoji_list == []:\n",
    "        return text\n",
    "    for num in range(len(emoji_list)-1,-1,-1):\n",
    "        i = emoji_list[num]['location']\n",
    "        x = emoji_list[num]['emoji']\n",
    "        new_text = text[:i] + ' ' + text[i:i+1] + ' ' + new_text[i+1:]\n",
    "        new_text = re.sub('[\\.\\,\\!\\?\\:\\;\\-\\=\\_\\'\\%\\(\\)\\¯\\\\\\¯]', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_spaced_emoji = [spaced_emoji(x) for x in notes_r1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/spaced_notes.pickle', 'wb') as f:\n",
    "    pickle.dump(notes_spaced_emoji, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined_Notes\n",
    "Add emoji translations to the text, so that I can associate emojis with certain words. That way those notes with emojis only or those with text only can start to be grouped together\n",
    "\n",
    "This may train the model to associate words with emojis. which may help with topic modeling, and getting popcorn and 🍿 into the same topics without needing an additional pre-processing step during production. Also, would be helpful for text generation, if we want to create an emoji suggestor based on various words/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defined_emoji(text):\n",
    "    emoji_list = emoji.emoji_lis(text)\n",
    "    new_text = str()\n",
    "    if emoji_list == []:\n",
    "        return text\n",
    "    for num in range(len(emoji_list)-1,-1,-1):\n",
    "        i = emoji_list[num]['location']\n",
    "        x = emoji_list[num]['emoji']\n",
    "        new_text = text[:i] + ' ' + text[i:i+1] + ' ' +emoji.demojize(x) + ' ' + new_text[i+1:]\n",
    "        new_text = re.sub('[\\.\\,\\!\\?\\:\\;\\-\\=\\_\\'\\%\\(\\)\\¯\\\\\\¯]', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 🍿  popcorn   🍿  popcorn   🍿  popcorn  '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of defined_emoji\n",
    "emoji_text = '🍿🍿🍿'\n",
    "\n",
    "defined_emoji(emoji_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_notes = [defined_emoji(x) for x in notes_r1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/defined_notes.pickle', 'wb') as f:\n",
    "    pickle.dump(defined_notes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single_Defined\n",
    "same as defined_notes, however this does not add spaces between the emojis. Without spaces, then the emoji and its definition will be considered one word. This may help to consider emojis separately from those who spell it out(hamburger vs the emoji hamburger). Could be useful when emojis take on a slightly different meaning/usage than the demojized word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_defined_emoji(text):\n",
    "    emoji_list = emoji.emoji_lis(text)\n",
    "    new_text = str()\n",
    "    if emoji_list == []:\n",
    "        return text\n",
    "    for num in range(len(emoji_list)-1,-1,-1):\n",
    "        i = emoji_list[num]['location']\n",
    "        x = emoji_list[num]['emoji']\n",
    "        new_text = text[:i] + ' ' + text[i:i+1] + ' ' +emoji.demojize(x) + ' ' + new_text[i+1:]\n",
    "        new_text = re.sub('[\\.\\,\\!\\?\\:\\;\\-\\=\\'\\%\\(\\)\\¯\\\\\\¯]', '', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_defined_notes = [single_defined_emoji(x) for x in notes_r1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/single_defined_notes.pickle', 'wb') as f:\n",
    "    pickle.dump(single_defined_notes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demoji_notes\n",
    "\n",
    "### demoji + remove foreign characters\n",
    " - demojize the note, remove punctuation\n",
    " - considered to remove non-latin characters, but since it is small part of data set, instead just set a minimum number in the vectorizer and that will get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic demojizer with underscore removal\n",
    "def demojize_clean(text):\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub('[\\.\\,\\!\\?\\:\\;\\-\\=\\_\\'\\%\\(\\)\\¯\\\\\\¯]', ' ', text)\n",
    "    return text\n",
    "# reference: https://stackoverflow.com/questions/23680976/python-removing-non-latin-characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoji_notes = [demojize_clean(x) for x in notes_r1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/demoji_notes.pickle', 'wb') as f:\n",
    "    pickle.dump(demoji_notes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic demojizer with underscore removal\n",
    "def single_demojize_clean(text):\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub('[\\.\\,\\!\\?\\:\\;\\-\\=\\_\\'\\%\\(\\)\\¯\\\\\\¯]', '', text)\n",
    "    return text\n",
    "# reference: https://stackoverflow.com/questions/23680976/python-removing-non-latin-characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_demoji_notes = [single_demojize_clean(x) for x in notes_r1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/single_demoji_notes.pickle', 'wb') as f:\n",
    "    pickle.dump(single_demoji_notes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAMPION PROCESSING\n",
    "- Converts all emojis into words (demojis)\n",
    "- Custom changes to the words/demojis so that word embeddings from Google-news-300 can be used.\n",
    "- Note that in the Word_Vector_matrix file, additional special processing will be done where **new** words such as \"lyft\" or \"venmo\" will be have embeddings from replacement words (\"cab\" or \"paypal\") because the Google-news-300 dataset is from 2012, prior to the newer technologies being adopted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep demojis with the underscore\n",
    "def champion_demojize_clean(text):\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    #custom definition for interprebility against Google training set\n",
    "    text = text.replace('hot_dog', 'hotdog')\n",
    "    text = text.replace('hot_beverage', 'coffee') #this \n",
    "    text = text.replace('sport_utility_vehicle', 'SUV') \n",
    "    text = text.replace('shallow_pan_of_food', 'paella')  #https://emojipedia.org/shallow-pan-of-food/\n",
    "    text = text.replace('cup_of_water', 'soda')\n",
    "    text = text.replace('money_with_wings', 'money')\n",
    "    text = text.replace('wrapped_gift', 'gift')\n",
    "\n",
    "    #remove punctuations, especially key to get rid of semicolons and underscores\n",
    "    text = re.sub('[\\.\\,\\!\\?\\:\\;\\-\\=\\_\\'\\%\\(\\)\\¯\\\\\\¯]', ' ', text)\n",
    "    \n",
    "    #add back underscore for connected words/emojis that Google data can understand\n",
    "    text = text.replace('french fries', 'french_fries')\n",
    "    text = text.replace('bento box', 'bento_box')\n",
    "    text = text.replace('light bulb', 'light_bulb')\n",
    "    text = text.replace('beer mugs', 'beer_mugs')\n",
    "    text = text.replace('clinking glasses', 'clinking_glasses')\n",
    "    text = text.replace('steaming bowl', 'steaming_bowl')\n",
    "    text = text.replace('fried shrimp', 'fried_shrimp')\n",
    "    text = text.replace('santa claus', 'santa_claus')\n",
    "    text = text.replace('palm tree', 'palm_tree')\n",
    "    text = text.replace('cherry blossom', 'cherry_blossom')\n",
    "    \n",
    "    #custom definition for interprebility (done after the fact because of the underscore)\n",
    "    text = text.replace('poultry leg', 'chicken_drumstick')\n",
    "    \n",
    "    \n",
    "    #custom stop words\n",
    "    text = text.replace('oncoming', '') #oncoming_automobile\n",
    "    text = text.replace('selector', '') #airplane_selector red_heart_selector      \n",
    "\n",
    "    #other notes:\n",
    "    # dog_face and pig_face will generate the terms: dog, pig, face. leaving this as is to get the face\n",
    "    # ok_hand will become ok, hand. Then ok will be removed in later vectorizer option for only 3 letter words.\n",
    "    # other strange emoji definitions: telephone_receiver, zany_hearts, rocket_popsicle. last of which is a custom venmo made for festivals\n",
    "    \n",
    "    return text\n",
    "# reference: https://stackoverflow.com/questions/23680976/python-removing-non-latin-characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_demoji_notes = [champion_demojize_clean(x) for x in notes_r1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/champion_demoji_notes.pickle', 'wb') as f:\n",
    "    pickle.dump(champion_demoji_notes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38083"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "champion_demoji_notes.index('bnb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove foreign languages -DEPRECATED IDEA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I attempted to remove foreign languages using the langdetect library. \n",
    "\n",
    "However, this method was ultimately not useful because many documents (transaction notes) showed up as other languages such as de (because of uber)\n",
    "\n",
    "still, had some learnings:\n",
    "1. Emoji only notes are the most common type of notes by far (interpreted as 'None' language)\n",
    "    - therefore it is very important to account for emojis properly\n",
    "2. Every transaction must have something in the note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n"
     ]
    }
   ],
   "source": [
    "#note that detect algorithm is slow to run. Took about one hour to run this on the 280k dataset\n",
    "lang = []\n",
    "counter = 0\n",
    "for x in df.note:\n",
    "    try:\n",
    "        lang.append(detect(x))\n",
    "    except:\n",
    "        lang.append('None')\n",
    "    if counter %1000 == 0:\n",
    "        print(counter)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_series = pd.DataFrame({'language':lang})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_frame = pd.concat([df.note,lang_series],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [note, language]\n",
       "Index: []"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are no empty notes\n",
    "note_frame[note_frame.note=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None     92169\n",
       "en       76680\n",
       "de       17316\n",
       "so        7168\n",
       "tl        6662\n",
       "af        6346\n",
       "fr        6261\n",
       "it        5659\n",
       "cy        5175\n",
       "id        5087\n",
       "ro        4764\n",
       "nl        4762\n",
       "da        4135\n",
       "ca        3836\n",
       "no        3832\n",
       "tr        3710\n",
       "sv        3608\n",
       "es        3414\n",
       "sw        3087\n",
       "vi        2840\n",
       "pt        2827\n",
       "pl        2726\n",
       "et        2656\n",
       "fi        2595\n",
       "lt        1535\n",
       "sq        1504\n",
       "sl        1411\n",
       "sk         991\n",
       "hr         964\n",
       "lv         862\n",
       "hu         770\n",
       "cs         688\n",
       "ko         152\n",
       "ja         141\n",
       "zh-cn       69\n",
       "el          13\n",
       "ru          11\n",
       "ar           8\n",
       "he           7\n",
       "uk           7\n",
       "fa           4\n",
       "th           3\n",
       "bg           3\n",
       "ne           3\n",
       "zh-tw        1\n",
       "ur           1\n",
       "mk           1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_frame.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "None     [💇, 👿, 🎳, 🍵, ⛽, ✊🏿, 💃, 🌮🌮, 🥑, 🙏🏼, 🔥🍗, 🐔, 🎫, 🍿,...\n",
       "af       [Hawt sliders, Jackie hotel 🏨, water, water!, ...\n",
       "ar       [فووووووووود, صصسييدزف يحييك كان. جلسة سنشري س...\n",
       "bg                        [Борщ, Жранина🐓🐠🍷🌷, Битва за 4$]\n",
       "ca       [Tequilla, Kids a square, formal (x2), Electri...\n",
       "cs       [Pickles, Photos, Love you 💜💎💍📿🌴🌞💙, Mcdicks🍟, ...\n",
       "cy       [Alvin’s dynasty $, dynasty, Mod, Gold, Legall...\n",
       "da       [Tbell, Evening fun, Dave, Jeremy, 🏐 refund, H...\n",
       "de       [FF, sUpReMe, :uber:, SF, Lunch!, Dnr3, Dinna,...\n",
       "el       [Ρ, ΥΟΓΓ, 🚗ΔΓ, 🍻  QTπ, Γοω τΗε βοατ, ( ^ω^ ), ...\n",
       "en       [Food, Chack, Toyota. The whole company, She b...\n",
       "es       [EScaPe, fugacious 💩, Electric, Ague para/por ...\n",
       "et       [Massage, Test, Shakes 🥤, Shakes! 🧉, Shakes🥤, ...\n",
       "fa             [دجاج, ᕦ(ò_óˇ)ᕤ٩( ᐛ )و(￣^￣)ゞ🍣🍣, ها, مارينا]\n",
       "fi       [Tattoo, ☕️ and jimmy johns, Kennison💗, Kennis...\n",
       "fr       [Jesse!!, Lo, Lauren Daigle tickets!, Lrc, Ken...\n",
       "he       [גוי 🤮, מעגל, ח, יין 🎊, ‏סתומה, שמואל בן אברהם...\n",
       "hr       [Kusinski squares, Purim palooza, Brap brap, C...\n",
       "hu       [Hobbz, Cozyyyyy vibes, Vball bb, Hi Abbott, A...\n",
       "id       [Jandon, Yup, Yeah I’m, Sum, Baseball Game, Da...\n",
       "it       [Sonora, Go puffs, Girl stuff, Overtime goal, ...\n",
       "ja       [¯\\_(ツ)_/¯, さいごのタイムがなてるよ, /    イ              ...\n",
       "ko       [스미마셍 이자 붙임, 快乐, 零食, 我要去昆明呀～, 男A 女免, 按摩手法可以的 下...\n",
       "lt       [Xmas party, Drinks, Snapsgiving, Rent minus p...\n",
       "lv       [Airbnb!, Gas!!!!, Tara Oils 🌿🌿🌿, pats, Lark, ...\n",
       "mk                          [Жорево и нумеровка тактов🌷🌷🌷]\n",
       "ne                                                    [ने]\n",
       "nl       [Poop, Pop 👁, Hot tub, Hi, Em- Hotel, WeeFee, ...\n",
       "no       [I forgot lol, Funnel, Shakes and energy!, Sha...\n",
       "pl       [B :rownie, I won, Zazazazaza, Lego, Za, Back,...\n",
       "pt       [For mariah, O, I do adore a jug, For soda fun...\n",
       "ro       [Groceries, late fees, Little Mumbai x 2, Jump...\n",
       "ru       [Яеит, 🐟🌮X2:, урок музыки, 🍷X12 🤣, Ниеггр, Спа...\n",
       "sk       [Movie 🎫, 🏈 pickem, Artichokes, Much love 💕, N...\n",
       "sl       [Poker, Knot, Man love, 💃🏼👒 (no sombrero emoji...\n",
       "so       [Nah, you good bro, Foooood, Foood, Whoooooooo...\n",
       "sq       [Shave, Trash, Fish 🐠, Hshshahajshhehehehshshe...\n",
       "sv       [Kaskade 2021, Tickets, Melker Karlsson, Snack...\n",
       "sw       [MERCH, WiFi, KICK, Wifi, Mya, Jambalaya, Subw...\n",
       "th           [คนเรา/สะยๆลๆลกวดเสกวไวๆยดรดดนำพวกพวกวก, พวก]\n",
       "tl       [Lion king, P, Okay, Parking spot, PT, Easy, K...\n",
       "tr       [Yeah, Yum yum favours, G yo, :venmo_dollar:, ...\n",
       "uk       [Стікер на машину, ค ๓ є г є ภ є ɭ є ς Շ г เ ς...\n",
       "ur                                          [ميسوري مينسك]\n",
       "vi       [chack, Q, TV, Phone 💵💵💸, Hh, Ty, Tv, Pho, T, ...\n",
       "zh-cn    [华人, 。, 话费, 优步, 谢谢哥👍, 桌游, 妥妥&苏益, 谢谢林老板, 好好练啊, ...\n",
       "zh-tw                                                  [豚]\n",
       "Name: note, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_frame.groupby('language')['note'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to remove the documents with following languages 'ar', 'bg', 'el','fa','he','ja','ko''mk','ne','ru','th','uk','ur','zh-cn','zh-tw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a little unfortunate to remove some tranasactions because those of a different language can still have emojis that provide information, or use text to show emotion such as ¯\\_(ツ)_/¯ is labeled as japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(note_frame.language.isin(['ar', 'bg', 'el','fa','he','ja','ko','mk','ne','ru','th','uk','ur','zh-cn','zh-tw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[note_frame.language.isin(['ar', 'bg', 'el','fa','he','ja','ko','mk','ne','ru','th','uk','ur','zh-cn','zh-tw'])==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrect - DEPRECATED\n",
    "\n",
    "Attempted Textblob for autocorrect but it is not good with double letters. Decided against autocorrect (specifically with Textblob and more generally with other libraries), because phones will typically have autocorrect built-in, so when a user enters a new transaction there should be few misspellings unless it is done on purpose (adding extra letters which i have addressed already in clean_round1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chin\n",
      "awwesome\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob('chikin')\n",
    "print(b.correct())\n",
    "c = TextBlob('awwesome')\n",
    "print(c.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunately, 'chikin', spelled incorrectly on purpose will just need to be that way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Database - NOT USED\n",
    "the code below is a work-in-progress. To be implemented in the future. Leaving this in here in case someone else is interested in using Google Cloud SQL Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install psycopg2\n",
    "# OR\n",
    "\n",
    "# pip install psycopg2-binary <- to avoid building from source which is what would be needed without the -binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cloud.google.com/sql/docs/postgres/connect-compute-engine#debianubuntu_2\n",
    "Before starting: Make sure that the SQL instance is set up in the same project as the JupyterNotebook instance\n",
    "\n",
    "- Follow instructions there, using Cloud SQL Proxy\n",
    "- When it asks to open up a terminal, this can be done in JupyterLab (File>New Launcher>Terminal)\n",
    "- step 6 - use the code for Debian/Ubuntu \n",
    "    - Can check this is the right OS within the terminal using \"cat /etc/os-release\" \n",
    "    - which i found here: https://www.cyberciti.biz/faq/how-to-check-os-version-in-linux-command-line/ because my guess was that the jupyter/python instance set up through GCP would be Linux.\n",
    "- step 7 - I used the Linux-64-bit code to install the proxy\n",
    "- step 8 - IN NEW TERMINAL - ran code in steps b and c\n",
    "    - note that for step c need to pull the instance name for the SQL instance\n",
    "        - because i set up the SQL instance in same project as JupyterNotebook, i dont need to deal with credentials\n",
    "- step 9 - INSTEAD OF RUNNING the psql in terminal, ran the connection through psycopg2 below.\n",
    "    - note that need to install psycopg2. this can be done through conda or pip...\n",
    "        - for pip use: \"pip install psycopg2-binary\"\n",
    "            - must use binary or it wont work here\n",
    "        - for conda use: \"conda install psycopg2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "import psycopg2.extras as extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#use following in a Terminal\n",
    "# ./cloud_sql_proxy -dir=/cloudsql -instances=regal-subject-286004:us-west2:venmo-database\n",
    "# or\n",
    "# ./cloud_sql_proxy -dir=/cloudsql &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pg.connect(host='/cloudsql/regal-subject-286004:us-west2:venmo-database',dbname=\"venmo\", user=\"postgres\", password=\"secret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[20000:20010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get everything into a SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to be used below\n",
    "def execute_values(conn, df, table,columns):\n",
    "    \"\"\"\n",
    "    Using psycopg2.extras.execute_values() to insert the dataframe\n",
    "    \"\"\"\n",
    "    # Create a list of tuples from the dataframe values\n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    # Comma-separated dataframe columns\n",
    "    cols = ','.join(list(df[columns]))\n",
    "    # SQL quert to execute\n",
    "    query  = \"INSERT INTO %s(%s) VALUES %%s\" % (table, cols)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        extras.execute_values(cursor, query, tuples)\n",
    "        conn.commit()\n",
    "    except (Exception, pg.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    print(\"execute_values() done\")\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_values(conn,df,'transactions',df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.iloc[:,1])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m54"
  },
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
